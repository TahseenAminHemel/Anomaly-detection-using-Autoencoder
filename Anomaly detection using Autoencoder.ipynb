{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9f0nj-fUHg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8efae4ae-d105-4b83-fd2e-60d9b84bef0c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuD-PDAyUehM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "576b7102-0fd8-4c1c-d408-eb066e5cc729"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjkSsJ7mUe_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MRP/creditcard.csv')\n",
        "temp = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MRP/creditcard.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPpbgy-nS-2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3e543099-b9f9-4f4e-9969-eac31e8a4f8c"
      },
      "source": [
        "data.groupby('Class')['Class'].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Class\n",
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVLOp_sJytzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "323938b7-b956-4d7f-ef20-59325f88a3ae"
      },
      "source": [
        "legitimate_mask = data['Class'] == 0\n",
        "anomaly_mask = data['Class'] != 0\n",
        "\n",
        "data.drop ('Class', axis = 1, inplace = True)\n",
        "\n",
        "data_legitimate = data[legitimate_mask]\n",
        "data_anomaly = data[anomaly_mask]\n",
        "\n",
        "print(f\"Legitimate Count: {len(data_legitimate)}\")\n",
        "print(f\"Anomaly Count: {len(data_anomaly)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Legitimate Count: 284315\n",
            "Anomaly Count: 492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETmk3KcCzzPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_legitimate = data_legitimate.values\n",
        "X_anamoly = data_anomaly.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W28D3cuQ0RAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_legitimate_train, X_legitimate_test = train_test_split(X_legitimate, test_size = 0.25, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsKxgwvd_bw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = temp.drop('Class', axis = 1).values\n",
        "y = temp['Class'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOtMLnmA0RJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91477eb7-18b3-49f2-c155-d9f909ee793e"
      },
      "source": [
        "print(f\"legitimate train Count: {len(X_legitimate_train)}\")\n",
        "print(f\"legitimate test Count: {len(X_legitimate_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "legitimate train Count: 213236\n",
            "legitimate test Count: 71079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mATpc9GE0RQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8e045b4-c0ac-40f1-9b29-716184745b84"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import models, layers\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyRb-3dc0RUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c316001a-c618-4056-fb94-8d16c425d4ad"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim = X_legitimate_train.shape[1], activation = 'relu'))\n",
        "model.add(Dense(3, activation = 'relu'))\n",
        "model.add(Dense(25, activation = 'relu'))\n",
        "model.add(Dense(X_legitimate_train.shape[1]))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(X_legitimate_train, X_legitimate_train, batch_size = 10240, verbose = 1, epochs = 200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "213236/213236 [==============================] - 0s 2us/step - loss: 366298750.8853\n",
            "Epoch 2/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 338784537.9402\n",
            "Epoch 3/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 282393884.5322\n",
            "Epoch 4/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 196717309.2604\n",
            "Epoch 5/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 98215352.5679\n",
            "Epoch 6/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 20048815.1726\n",
            "Epoch 7/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 416777.5690\n",
            "Epoch 8/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 218223.9042\n",
            "Epoch 9/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 11247.4034\n",
            "Epoch 10/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 4719.3262\n",
            "Epoch 11/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3145.7760\n",
            "Epoch 12/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2921.2702\n",
            "Epoch 13/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2865.4341\n",
            "Epoch 14/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3347.1042\n",
            "Epoch 15/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2492.8475\n",
            "Epoch 16/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2144.6911\n",
            "Epoch 17/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6416.1335\n",
            "Epoch 18/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2528.2318\n",
            "Epoch 19/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1757.9841\n",
            "Epoch 20/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1387.6005\n",
            "Epoch 21/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 766.5757\n",
            "Epoch 22/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 210.8241\n",
            "Epoch 23/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 37.5888\n",
            "Epoch 24/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 15.9444\n",
            "Epoch 25/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 12.1853\n",
            "Epoch 26/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 10.4654\n",
            "Epoch 27/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 10.9740\n",
            "Epoch 28/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 15.9659\n",
            "Epoch 29/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 8.1015\n",
            "Epoch 30/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6.8410\n",
            "Epoch 31/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 22.8372\n",
            "Epoch 32/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 17.8719\n",
            "Epoch 33/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 26.5586\n",
            "Epoch 34/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 8.3372\n",
            "Epoch 35/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 28.0574\n",
            "Epoch 36/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 67.3630\n",
            "Epoch 37/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 28.7340\n",
            "Epoch 38/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 7.6710\n",
            "Epoch 39/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.8219\n",
            "Epoch 40/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 18.7971\n",
            "Epoch 41/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 32.5606\n",
            "Epoch 42/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 20.0961\n",
            "Epoch 43/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 70.2345\n",
            "Epoch 44/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 21.0108\n",
            "Epoch 45/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 7.0529\n",
            "Epoch 46/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 81.7969\n",
            "Epoch 47/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 23.4549\n",
            "Epoch 48/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.5545\n",
            "Epoch 49/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 58.9002\n",
            "Epoch 50/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 35.8658\n",
            "Epoch 51/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 127.8179\n",
            "Epoch 52/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 76.7448\n",
            "Epoch 53/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 11.8604\n",
            "Epoch 54/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 4.2948\n",
            "Epoch 55/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 13.8396\n",
            "Epoch 56/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 30.8289\n",
            "Epoch 57/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 15.2873\n",
            "Epoch 58/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 134.5990\n",
            "Epoch 59/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 23.1779\n",
            "Epoch 60/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 4.7299\n",
            "Epoch 61/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 82.3488\n",
            "Epoch 62/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 111.1425\n",
            "Epoch 63/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 30.2367\n",
            "Epoch 64/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.7403\n",
            "Epoch 65/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 33.1525\n",
            "Epoch 66/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 254.5911\n",
            "Epoch 67/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 41.3493\n",
            "Epoch 68/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 8.2647\n",
            "Epoch 69/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 46.9573\n",
            "Epoch 70/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 27.7846\n",
            "Epoch 71/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 8.7149\n",
            "Epoch 72/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 278.2177\n",
            "Epoch 73/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 76.4529\n",
            "Epoch 74/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 11.1325\n",
            "Epoch 75/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.3134\n",
            "Epoch 76/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 81.8915\n",
            "Epoch 77/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 15.5332\n",
            "Epoch 78/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 54.0047\n",
            "Epoch 79/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 355.8524\n",
            "Epoch 80/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 30.6182\n",
            "Epoch 81/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.6005\n",
            "Epoch 82/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 43.7913\n",
            "Epoch 83/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 51.2698\n",
            "Epoch 84/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.9097\n",
            "Epoch 85/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3.1922\n",
            "Epoch 86/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 33.2047\n",
            "Epoch 87/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 646.3491\n",
            "Epoch 88/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 93.9311\n",
            "Epoch 89/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 14.8697\n",
            "Epoch 90/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3.2788\n",
            "Epoch 91/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 23.1237\n",
            "Epoch 92/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 51.3970\n",
            "Epoch 93/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 13.7223\n",
            "Epoch 94/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.4358\n",
            "Epoch 95/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1.6057\n",
            "Epoch 96/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.5183\n",
            "Epoch 97/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 253.2717\n",
            "Epoch 98/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 71.9206\n",
            "Epoch 99/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 12.6319\n",
            "Epoch 100/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6.5679\n",
            "Epoch 101/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 173.6846\n",
            "Epoch 102/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 295.7931\n",
            "Epoch 103/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 29.3010\n",
            "Epoch 104/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 4.9377\n",
            "Epoch 105/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.6844\n",
            "Epoch 106/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 63.4187\n",
            "Epoch 107/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 13.7812\n",
            "Epoch 108/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 39.9297\n",
            "Epoch 109/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 304.7930\n",
            "Epoch 110/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 43.3362\n",
            "Epoch 111/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 26.4701\n",
            "Epoch 112/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 70.7532\n",
            "Epoch 113/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6.1614\n",
            "Epoch 114/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 53.9250\n",
            "Epoch 115/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 274.3902\n",
            "Epoch 116/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 30.5309\n",
            "Epoch 117/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 10.0264\n",
            "Epoch 118/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 143.0221\n",
            "Epoch 119/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 23.4422\n",
            "Epoch 120/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 18.3289\n",
            "Epoch 121/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 541.3958\n",
            "Epoch 122/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 73.6464\n",
            "Epoch 123/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.5304\n",
            "Epoch 124/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.3290\n",
            "Epoch 125/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1.5164\n",
            "Epoch 126/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 92.4309\n",
            "Epoch 127/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 28.4311\n",
            "Epoch 128/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6.3144\n",
            "Epoch 129/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.8339\n",
            "Epoch 130/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 200.9491\n",
            "Epoch 131/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 358.0377\n",
            "Epoch 132/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 33.5948\n",
            "Epoch 133/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.2764\n",
            "Epoch 134/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 7.4996\n",
            "Epoch 135/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 146.0434\n",
            "Epoch 136/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 19.5309\n",
            "Epoch 137/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3.4242\n",
            "Epoch 138/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.2471\n",
            "Epoch 139/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 433.8804\n",
            "Epoch 140/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 57.9056\n",
            "Epoch 141/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.0820\n",
            "Epoch 142/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3.8207\n",
            "Epoch 143/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 113.5532\n",
            "Epoch 144/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 37.8562\n",
            "Epoch 145/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.4764\n",
            "Epoch 146/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 165.2750\n",
            "Epoch 147/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 280.2434\n",
            "Epoch 148/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 21.2291\n",
            "Epoch 149/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 4.7508\n",
            "Epoch 150/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1.9334\n",
            "Epoch 151/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 154.8825\n",
            "Epoch 152/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 67.4362\n",
            "Epoch 153/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 18.5285\n",
            "Epoch 154/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 298.2982\n",
            "Epoch 155/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 104.2287\n",
            "Epoch 156/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 14.8526\n",
            "Epoch 157/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.6376\n",
            "Epoch 158/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.5226\n",
            "Epoch 159/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 248.2158\n",
            "Epoch 160/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 63.8446\n",
            "Epoch 161/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 101.8312\n",
            "Epoch 162/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 113.7052\n",
            "Epoch 163/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 13.3144\n",
            "Epoch 164/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 7.6129\n",
            "Epoch 165/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 296.6722\n",
            "Epoch 166/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 157.2200\n",
            "Epoch 167/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 36.6489\n",
            "Epoch 168/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 5.4382\n",
            "Epoch 169/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 73.2526\n",
            "Epoch 170/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 64.5666\n",
            "Epoch 171/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 13.9911\n",
            "Epoch 172/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 300.2927\n",
            "Epoch 173/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 58.8007\n",
            "Epoch 174/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 8.3736\n",
            "Epoch 175/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 78.4057\n",
            "Epoch 176/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 171.8160\n",
            "Epoch 177/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 24.5014\n",
            "Epoch 178/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 32.9025\n",
            "Epoch 179/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 404.8880\n",
            "Epoch 180/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 54.4232\n",
            "Epoch 181/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 9.9038\n",
            "Epoch 182/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 3.0225\n",
            "Epoch 183/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.0551\n",
            "Epoch 184/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 175.7478\n",
            "Epoch 185/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 416.0252\n",
            "Epoch 186/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 52.1761\n",
            "Epoch 187/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 6.8082\n",
            "Epoch 188/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1.8981\n",
            "Epoch 189/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 1.3047\n",
            "Epoch 190/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 41.0887\n",
            "Epoch 191/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 398.6192\n",
            "Epoch 192/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 32.4928\n",
            "Epoch 193/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 11.5236\n",
            "Epoch 194/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 2.5346\n",
            "Epoch 195/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 14.0951\n",
            "Epoch 196/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 398.7257\n",
            "Epoch 197/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 65.6704\n",
            "Epoch 198/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 52.7738\n",
            "Epoch 199/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 73.3787\n",
            "Epoch 200/200\n",
            "213236/213236 [==============================] - 0s 1us/step - loss: 218.4188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6849664f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZxo5Rk8BnDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cc4b3c2b-1740-4ddd-b0ab-254d72ec3181"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 25)                775       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 78        \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 25)                100       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 30)                780       \n",
            "=================================================================\n",
            "Total params: 1,733\n",
            "Trainable params: 1,733\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2uMN_eHv0ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "36606fab-f241-4c4f-f6f6-a1a04d2fef2e"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "predictions = model.predict_classes(X_test)\n",
        "predictions = np.where(predictions > 0, 1, 0)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     71077\n",
            "           1       0.00      0.00      0.00       125\n",
            "\n",
            "    accuracy                           1.00     71202\n",
            "   macro avg       0.50      0.50      0.50     71202\n",
            "weighted avg       1.00      1.00      1.00     71202\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MvVvFVnwMF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "73034cba-f141-42a0-f1a4-51b46157decb"
      },
      "source": [
        "confusion_matrix(y_test,predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[71046,    31],\n",
              "       [  125,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aahpOtv0ILV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "81ef79ed-a762-432d-e11d-f8a561a99d04"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "pred = model.predict(X_legitimate_test)\n",
        "score1 = np.sqrt(metrics.mean_squared_error(pred, X_legitimate_test))\n",
        "print(f\"In Sample legitimate Score (RMSE): {score1}\".format(score1))\n",
        "\n",
        "pred = model.predict(X_legitimate)\n",
        "score2 = np.sqrt(metrics.mean_squared_error(pred, X_legitimate))\n",
        "print(f\"Out of Sample legitimate Score (RMSE): {score2}\")\n",
        "\n",
        "pred = model.predict(X_anamoly)\n",
        "score3 = np.sqrt(metrics.mean_squared_error(pred, X_anamoly))\n",
        "print(f\"Anomaly Score (RMSE): {score3}\".format(score1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Sample legitimate Score (RMSE): 8.666338493987373\n",
            "Out of Sample legitimate Score (RMSE): 8.668498826207781\n",
            "Anomaly Score (RMSE): 10.083318076323843\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}