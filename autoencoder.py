# -*- coding: utf-8 -*-
"""Copy of Autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UPAPqVKrzUslxzYRcf4TcTd3spE6puVr
"""

# importing necessary libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category = FutureWarning)

# Mounting the google drive

from google.colab import drive
drive.mount('/content/drive/')

# Loading the data in two Pandas dataframes

data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MRP/creditcard.csv')

# We split it into X_train, X_test, y_train, y_test for the purpose of classification_report and confusion_matrix.

from sklearn.model_selection import train_test_split

X = data.drop('Class', axis = 1).values
y = data['Class'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=101)

# We split the data agian here in two dataframes; legitimate and anomaly. Then we drop the 'Class' column from out original 'data' 
# and take all the legitimate transactions in the data_legitimate variable and the anomalies in the data_anomaly variable.
# That is why this is unsupervised.

legitimate = data['Class'] == 0
anomaly = data['Class'] != 0

data.drop ('Class', axis = 1, inplace = True)
data_legitimate = data[legitimate]
data_anomaly = data[anomaly]

data_legitimate_train, data_legitimate_test = train_test_split(data_legitimate, test_size = 0.2, random_state=101)

print(f"Legitimate Count: {len(data_legitimate)}")
print(f"Anomaly Count: {len(data_anomaly)}")

print(f"Legitimate train Count: {len(data_legitimate_train)}")
print(f"Legitimate test Count: {len(data_legitimate_test)}")

# importing the necessary libraries

import tensorflow as tf
from keras import models, layers
from keras.layers import Dense
from keras.models import Sequential

# defnining the autoencoder and compiling the model and fitting the model with data_legitimate_train only

model = Sequential()
model.add(Dense(50, input_dim = data_legitimate_train.shape[1], activation = 'relu'))
model.add(Dense(5, activation = 'relu'))
model.add(Dense(50, activation = 'relu'))
model.add(Dense(data_legitimate_train.shape[1]))

model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(data_legitimate_train, data_legitimate_train, verbose = 1, epochs = 200)

# The autoencoder model summary

model.summary()

# The classification report for the autoencoder

from sklearn.metrics import classification_report, confusion_matrix
predictions = model.predict_classes(X_test)
predictions = np.where(predictions > 0, 1, 0)
print(classification_report(y_test, predictions))

# The consusion matrix for the autoencoder

confusion_matrix(y_test,predictions)

# The RMSE score for legitimate and anomaly section of the whole data

from sklearn import metrics

pred = model.predict(data_legitimate_test)
score1 = np.sqrt(metrics.mean_squared_error(pred, data_legitimate_test))
print(f"data_legitimate_test Score (RMSE): {score1}".format(score1))

pred = model.predict(data_legitimate)
score2 = np.sqrt(metrics.mean_squared_error(pred, data_legitimate))
print(f"data_legitimate Score (RMSE): {score2}")

pred = model.predict(data_anomaly)
score3 = np.sqrt(metrics.mean_squared_error(pred, data_anomaly))
print(f"anomaly Score (RMSE): {score3}".format(score1))